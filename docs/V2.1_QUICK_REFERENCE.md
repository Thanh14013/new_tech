# V2.1 Quick Reference Card
## Essential Formulas, Code Snippets & Decision Rules

**Version:** 2.1  
**Date:** January 21, 2026  

---

## ðŸŽ¯ FOUR PILLAR TECHNIQUES

### 1ï¸âƒ£ TWO-STAGE HURDLE MODEL

**When to use:** Always (handles 95% zero-inflation)

**Formula:**
```
LTV_predicted = P(is_payer) Ã— E[Amount | is_payer]
```

**Code:**
```python
# Stage 1: Classification
clf = XGBClassifier(scale_pos_weight=20, max_depth=5)
clf.fit(X_d1, y_is_payer_d60)
prob_payer = clf.predict_proba(X_new)[:, 1]

# Stage 2: Regression (payers only)
X_payers = X_d1[y_is_payer_d60 == 1]
reg = XGBRegressor(max_depth=6)
reg.fit(X_payers, y_ltv_d60[y_is_payer_d60 == 1])
amount = reg.predict(X_new)

# Combine
ltv_final = prob_payer * amount
```

**Key Parameters:**
- `scale_pos_weight`: (# non-payers) / (# payers) â‰ˆ 20
- Stage 1 depth: 5 (simpler for classification)
- Stage 2 depth: 6 (more complex for regression)

---

### 2ï¸âƒ£ SEMANTIC SIMILARITY MAPPING

**When to use:** For new campaigns (no training data)

**Formula:**
```
similarity = cosine_similarity(new_campaign_vector, training_vectors)
if max(similarity) > 0.6:
    use matched_campaign_model
else:
    fallback to app_level_model
```

**Code:**
```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Build index
vectorizer = TfidfVectorizer(ngram_range=(2,3), analyzer='char_wb')
train_vectors = vectorizer.fit_transform(training_campaigns)

# Find match
new_vector = vectorizer.transform([new_campaign])
similarities = cosine_similarity(new_vector, train_vectors)[0]

best_match_idx = similarities.argmax()
best_score = similarities[best_match_idx]

if best_score >= 0.6:
    model = load_model(training_campaigns[best_match_idx])
else:
    model = load_app_model(app_id)
```

**Threshold Rules:**
- similarity >0.8: High confidence match
- similarity 0.6-0.8: Medium confidence match
- similarity <0.6: No match, use fallback

---

### 3ï¸âƒ£ BAYESIAN PRIORS FOR CURVE FITTING

**When to use:** Low-data campaigns (<500 rows)

**Formula:**
```
Objective = MSE(y, Å·) + Î» Ã— [(a - a_prior)Â²/ÏƒÂ² + (b - b_prior)Â²/ÏƒÂ²]
where: Å· = a Ã— x^b
```

**Code:**
```python
from scipy.optimize import minimize

def fit_with_bayesian_prior(x, y, tier_priors):
    a_prior = tier_priors['a']
    b_prior = tier_priors['b']
    sigma = tier_priors['sigma']
    
    def objective(params):
        a, b = params
        y_pred = a * np.power(x, b)
        
        # Prediction error
        mse = np.mean((y - y_pred)**2)
        
        # Prior penalty
        prior_penalty = (
            ((a - a_prior) / sigma)**2 +
            ((b - b_prior) / sigma)**2
        )
        
        return mse + 0.5 * prior_penalty
    
    result = minimize(
        objective,
        x0=[a_prior, b_prior],
        bounds=[(0.01, 1.0), (0.1, 1.0)]
    )
    
    return result.x
```

**Prior Values (Typical):**
- Tier 1: a=0.08, b=0.45, Ïƒ=0.10
- Tier 2: a=0.06, b=0.40, Ïƒ=0.15
- Tier 3: a=0.04, b=0.35, Ïƒ=0.20

---

### 4ï¸âƒ£ CALIBRATION LAYER

**When to use:** Always (applies to all predictions)

**Formula:**
```
pred_calibrated = pred_raw Ã— (1 - bias_campaign) Ã— seasonal_mult Ã— tier_mult
```

**Code:**
```python
def calibrate_prediction(pred_raw, campaign_id, month, tier, bias_db):
    # Historical bias
    bias = bias_db.get(campaign_id, {
        'bias_d30': 0.0,
        'bias_d60': 0.0
    })
    
    # Seasonal multiplier
    seasonal_mult = {
        'jan': 1.0, 'feb': 0.95, 'mar': 0.95,
        'apr': 0.90, 'may': 0.90, 'jun': 0.85,
        'jul': 0.85, 'aug': 0.90, 'sep': 0.95,
        'oct': 1.0, 'nov': 1.05, 'dec': 1.15
    }[month]
    
    # Tier multiplier
    tier_mult = {1: 0.98, 2: 1.0, 3: 1.05}[tier]
    
    # Apply calibration
    pred_calibrated = (
        pred_raw *
        (1 - bias['bias_d60']) *
        seasonal_mult *
        tier_mult
    )
    
    return pred_calibrated
```

**Bias Update (Rolling):**
```python
def update_bias(campaign_id, pred, actual, bias_db, alpha=0.3):
    """Exponential moving average"""
    old_bias = bias_db.get(campaign_id, 0.0)
    error_rate = (pred - actual) / actual
    new_bias = (1 - alpha) * old_bias + alpha * error_rate
    bias_db[campaign_id] = new_bias
```

---

## ðŸŽ¯ DECISION TREE

### Which method to use for a campaign?

```
START: New campaign prediction

â”œâ”€ Has training data? (â‰¥300 rows)
â”‚  â”œâ”€ YES â†’ Continue to Tier check
â”‚  â””â”€ NO â†’ Use Semantic Similarity Mapping (Method 2)
â”‚
â”œâ”€ What Tier?
â”‚  â”œâ”€ Tier 1 (Stable, CV<1.5, â‰¥1000 rows)
â”‚  â”‚  â”œâ”€ Growth pattern clear? â†’ Curve Fitting + Bayesian Prior
â”‚  â”‚  â”œâ”€ Complex features? â†’ ML Multiplier
â”‚  â”‚  â”œâ”€ Similar users exist? â†’ Look-alike
â”‚  â”‚  â””â”€ Always try: Two-Stage Hurdle
â”‚  â”‚  â””â”€ SELECT: Best MAPE on validation
â”‚  â”‚
â”‚  â”œâ”€ Tier 2 (Medium, CV 1.5-2.5, 300-1000 rows)
â”‚  â”‚  â”œâ”€ Priority: Two-Stage Hurdle
â”‚  â”‚  â”œâ”€ Secondary: Look-alike
â”‚  â”‚  â””â”€ Fallback: ML Multiplier
â”‚  â”‚
â”‚  â””â”€ Tier 3 (Volatile, CV>2.5, <300 rows)
â”‚     â”œâ”€ Priority: Look-alike (borrow from similar)
â”‚     â”œâ”€ Secondary: Two-Stage Hurdle (conservative)
â”‚     â””â”€ Fallback: App-level model
â”‚
â””â”€ Apply Calibration Layer (ALWAYS)
```

---

## ðŸ“Š FEATURE CHECKLIST

### Core Features (Must Have)

```python
features_core = [
    # Revenue
    'rev_d1', 'rev_d0', 'rev_sum', 'rev_max',
    'rev_d0_d1_ratio', 'velocity_d0_d1',
    
    # Engagement (CRITICAL)
    'retention_d1', 'avg_session_time_d1',
    'avg_level_reached_d1', 'actions_per_session',
    'feature_usage_rate', 'social_engagement',
    
    # CPI Quality (V2.1 NEW)
    'actual_cpi', 'cpi_vs_campaign_avg', 'cpi_vs_app_avg',
    'cpi_tier', 'cpi_quality_score',
    
    # Hurdle Model (V2.1 NEW)
    'is_payer_d1',  # Binary for Stage 1
    
    # Metadata
    'install_month', 'geo_tier', 'campaign_type', 'campaign_source'
]
```

### Advanced Features (Per Campaign)

```python
features_advanced = [
    # Historical profile
    'avg_ltv_d30_historical', 'avg_growth_rate',
    'campaign_maturity_days', 'seasonal_multiplier',
    
    # Comparative
    'ltv_vs_app_avg', 'ltv_vs_campaign_cluster',
    'performance_percentile'
]
```

---

## ðŸ”¢ KEY METRICS & THRESHOLDS

### Model Selection

| Metric | Tier 1 Target | Tier 2 Target | Tier 3 Target |
|--------|---------------|---------------|---------------|
| **MAPE** | â‰¤ 4% | â‰¤ 6% | â‰¤ 10% |
| **RÂ²** | â‰¥ 0.7 | â‰¥ 0.6 | â‰¥ 0.5 |
| **Coverage** | 100% | 100% | 90%+ |

### Hurdle Model

| Metric | Stage 1 (Classification) | Stage 2 (Regression) |
|--------|--------------------------|----------------------|
| **Target** | AUC â‰¥ 0.75 | RÂ² â‰¥ 0.6 |
| **Precision** | â‰¥ 70% | N/A |
| **Recall** | â‰¥ 60% | N/A |

### Semantic Matching

| Similarity Score | Action | Confidence |
|------------------|--------|------------|
| â‰¥ 0.8 | Use matched model | High |
| 0.6 - 0.8 | Use matched model | Medium |
| < 0.6 | Use app-level fallback | Low |

### Calibration

| Bias Magnitude | Action |
|----------------|--------|
| < 5% | Normal calibration |
| 5-20% | Apply calibration + monitor |
| > 20% | Retrain model immediately |

---

## ðŸš€ PRODUCTION WORKFLOW

### Training Pipeline

```bash
# 1. Tier classification
python scripts/classify_campaign_tiers.py

# 2. Feature engineering
python scripts/build_features_v21.py --include_engagement --include_cpi

# 3. Train 4 methods per campaign
python scripts/train_multi_model.py \
    --methods hurdle,curve,ml,lookalike \
    --tiers 1,2,3

# 4. Build semantic index
python scripts/build_semantic_index.py --training_months T8-T11

# 5. Calculate calibration bias
python scripts/calculate_bias.py --val_month T11

# 6. Validate
python scripts/validate_v21.py --test_month T12
```

### Prediction Pipeline

```python
def predict_ltv_d60(user_d1_features, campaign_id):
    # 1. Check if campaign has model
    if has_training_data(campaign_id):
        model = load_best_model(campaign_id)
    else:
        # Semantic matching
        match = semantic_matcher.find_match(campaign_id)
        if match['similarity'] > 0.6:
            model = load_model(match['matched_campaign'])
        else:
            model = load_app_model(get_app_id(campaign_id))
    
    # 2. Predict (may be hurdle model)
    pred_raw = model.predict(user_d1_features)
    
    # 3. Calibrate
    pred_final = calibrator.calibrate(
        pred_raw,
        campaign_id=campaign_id,
        month=user_d1_features['install_month'],
        tier=get_tier(campaign_id)
    )
    
    return pred_final
```

---

## âš ï¸ COMMON PITFALLS

### 1. Forgetting Scale for Hurdle Model

âŒ **Wrong:**
```python
clf = XGBClassifier()  # Imbalanced classes!
```

âœ… **Correct:**
```python
pos_weight = (y == 0).sum() / (y == 1).sum()
clf = XGBClassifier(scale_pos_weight=pos_weight)
```

### 2. Using Raw Predictions Without Calibration

âŒ **Wrong:**
```python
return model.predict(X)  # No calibration!
```

âœ… **Correct:**
```python
pred_raw = model.predict(X)
return calibrator.calibrate(pred_raw, campaign_id, month)
```

### 3. Training Hurdle Stage 2 on All Data

âŒ **Wrong:**
```python
reg.fit(X, y_ltv)  # Includes zeros!
```

âœ… **Correct:**
```python
X_payers = X[y_is_payer == 1]
y_payers = y_ltv[y_is_payer == 1]
reg.fit(X_payers, y_payers)
```

### 4. Semantic Matching Without Threshold

âŒ **Wrong:**
```python
best_match = similarities.argmax()
return load_model(best_match)  # May be poor match!
```

âœ… **Correct:**
```python
best_score = similarities.max()
if best_score > 0.6:
    return load_model(best_match)
else:
    return load_app_model(app_id)
```

---

## ðŸ“‹ VALIDATION CHECKLIST

Before deploying a campaign model:

- [ ] **Data Quality**
  - [ ] â‰¥300 rows in training
  - [ ] No missing values in key features
  - [ ] CPI available for â‰¥95% users
  - [ ] Engagement metrics populated

- [ ] **Model Training**
  - [ ] All 4 methods trained
  - [ ] Cross-validation performed
  - [ ] Best method selected based on MAPE
  - [ ] Hurdle model: Stage 1 AUC â‰¥0.75

- [ ] **Calibration**
  - [ ] Historical bias calculated (2+ months)
  - [ ] Seasonal multipliers defined
  - [ ] Calibrated MAPE â‰¤ target (Tier-specific)

- [ ] **Fallback**
  - [ ] Semantic match tested (if new campaign)
  - [ ] App-level model available
  - [ ] Confidence score calculated

- [ ] **Production**
  - [ ] Model saved with metadata
  - [ ] Versioning tracked
  - [ ] Monitoring alerts configured

---

## ðŸŽ¯ SUCCESS CRITERIA

### Per Campaign

| Metric | Tier 1 | Tier 2 | Tier 3 |
|--------|--------|--------|--------|
| MAPE D30 | â‰¤ 3% | â‰¤ 5% | â‰¤ 8% |
| MAPE D60 | â‰¤ 4% | â‰¤ 6% | â‰¤ 10% |
| Coverage | 100% | 100% | 95%+ |

### Overall System

- [ ] 80%+ campaigns meet Tier targets
- [ ] 98%+ coverage (including new campaigns)
- [ ] Inference time <100ms
- [ ] Bias update runs monthly
- [ ] Model refresh runs monthly

---

**Quick Reference Version:** 1.0  
**Last Updated:** January 21, 2026  
**Print this for your desk! ðŸ“Œ**
