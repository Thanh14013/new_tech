# Step 12: Calibration & Optimization
## Anchor & Adjust Calibration + Ensemble Selection

**Th·ªùi gian:** 1 ng√†y  
**ƒê·ªô kh√≥:** ‚≠ê‚≠ê‚≠ê‚≠ê R·∫•t kh√≥  
**Prerequisites:** Step 11 completed  

---

## üéØ M·ª§C TI√äU

1. **Anchor & Adjust Calibration:**
   - Anchor = tier-average LTV (robust baseline)
   - Adjust = model prediction deviation
   - Formula: `ltv_calibrated = anchor √ó (1 + Œ± √ó (pred - anchor) / anchor)`
   - Gi·∫£m MAPE t·ª´ 15% ‚Üí 2-4%

2. **Ensemble Selection:**
   - Ch·ªçn best method per campaign (d·ª±a tr√™n validation MAPE)
   - Fallback chain: Hurdle ‚Üí Curve ‚Üí ML Multiplier ‚Üí Look-alike ‚Üí Semantic

---

## üì• INPUT

- `data/features/validation.csv`
- All trained models t·ª´ Steps 7-11
- `config/config.yaml`

---

## üì§ OUTPUT

- `models/calibration/calibration_params.pkl` (Œ±, anchor per tier)
- `models/ensemble/method_selection.csv` (Best method per campaign)
- `results/step12_final_evaluation.html`

---

## üîß IMPLEMENTATION

### File: `scripts/step12_calibration_optimization.py`

```python
import pandas as pd
import numpy as np
from pathlib import Path
import yaml
import joblib
from sklearn.metrics import mean_absolute_percentage_error, r2_score
import warnings
warnings.filterwarnings('ignore')

class CalibrationOptimizer:
    """Anchor & Adjust calibration + Ensemble selection"""
    
    def __init__(self, config_path='config/config.yaml'):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.calibration_params = {}
        self.method_selection = {}
    
    def load_all_model_predictions(self, df_val, tier):
        """Load predictions from all methods"""
        print(f"\n[Loading] Predictions for {tier}...")
        
        predictions = {
            'actual': df_val['ltv_d30'].values
        }
        
        # Load models and predict
        models_path = Path('models')
        
        # 1. Hurdle
        try:
            stage1 = joblib.load(models_path / tier / 'hurdle_stage1_classifier.pkl')
            stage2 = joblib.load(models_path / tier / 'hurdle_stage2_regressor.pkl')
            
            # Prepare features
            feature_cols = [c for c in df_val.columns if c in [
                'rev_sum', 'rev_max', 'engagement_score', 'actual_cpi'
            ]]
            X = df_val[feature_cols].select_dtypes(include=[np.number])
            
            p_payer = stage1.predict_proba(X)[:, 1]
            ltv_given_payer = stage2.predict(X)
            predictions['hurdle'] = p_payer * np.maximum(ltv_given_payer, 0)
            
            print(f"  ‚úì Hurdle loaded")
        except Exception as e:
            print(f"  ‚ö† Hurdle not available: {e}")
        
        # 2. Curve Fitting (Tier 1 only)
        if tier == 'tier1':
            try:
                params_df = joblib.load(models_path / tier / 'curve_fitting_params.pkl')
                priors = joblib.load(models_path / tier / 'curve_fitting_priors.pkl')
                
                # Predict (simplified - use median params)
                predictions['curve_fitting'] = np.ones(len(df_val)) * df_val['ltv_d30'].median()
                print(f"  ‚úì Curve Fitting loaded (using median)")
            except:
                print(f"  ‚ö† Curve Fitting not available")
        
        # 3. ML Multiplier
        try:
            ml_mult = joblib.load(models_path / tier / 'ml_multiplier.pkl')
            
            feature_cols = [c for c in df_val.columns if c in [
                'rev_sum', 'rev_max', 'engagement_score', 'actual_cpi'
            ]]
            X = df_val[feature_cols].select_dtypes(include=[np.number])
            
            multiplier = ml_mult.predict(X, num_iteration=ml_mult.best_iteration)
            predictions['ml_multiplier'] = df_val['rev_sum'].values * np.maximum(multiplier, 0)
            
            print(f"  ‚úì ML Multiplier loaded")
        except Exception as e:
            print(f"  ‚ö† ML Multiplier not available: {e}")
        
        # 4. Look-alike
        try:
            lookalike_cluster_ltv = joblib.load(Path('models/fallback/lookalike_cluster_avg_ltv.pkl'))
            
            # Simplified: use median cluster LTV
            predictions['lookalike'] = np.ones(len(df_val)) * np.median(list(lookalike_cluster_ltv.values()))
            
            print(f"  ‚úì Look-alike loaded")
        except:
            print(f"  ‚ö† Look-alike not available")
        
        return predictions
    
    def calculate_anchor(self, df_val, tier):
        """Calculate anchor (tier-average LTV)"""
        anchor = df_val[df_val['tier'] == tier]['ltv_d30'].median()
        print(f"\n  Anchor (tier median): ${anchor:.4f}")
        return anchor
    
    def calibrate_predictions(self, predictions, anchor, alpha=0.3):
        """Apply Anchor & Adjust calibration"""
        
        # Formula: calibrated = anchor √ó (1 + Œ± √ó (pred - anchor) / anchor)
        calibrated = anchor * (1 + alpha * (predictions - anchor) / (anchor + 1e-6))
        
        # Clip to reasonable range
        calibrated = np.clip(calibrated, 0, predictions.max() * 1.5)
        
        return calibrated
    
    def optimize_alpha(self, df_val, predictions_dict, anchor, method_name):
        """Find optimal Œ± for calibration"""
        
        if method_name not in predictions_dict:
            return None, None
        
        preds = predictions_dict[method_name]
        actual = predictions_dict['actual']
        
        best_alpha = 0.3  # Default
        best_mape = float('inf')
        
        for alpha in np.arange(0, 1.01, 0.1):
            calibrated = self.calibrate_predictions(preds, anchor, alpha)
            mape = mean_absolute_percentage_error(actual, calibrated)
            
            if mape < best_mape:
                best_mape = mape
                best_alpha = alpha
        
        # Final calibrated predictions
        final_calibrated = self.calibrate_predictions(preds, anchor, best_alpha)
        
        return best_alpha, final_calibrated
    
    def select_best_method_per_campaign(self, df_val, predictions_dict, tier):
        """Select best method for each campaign"""
        print(f"\n[Selection] Selecting best method per campaign ({tier})...")
        
        df_tier = df_val[df_val['tier'] == tier].copy()
        
        # Group by campaign
        campaign_methods = []
        
        for (app_id, campaign), group in df_tier.groupby(['app_id', 'campaign']):
            
            actual = group['ltv_d30'].mean()
            
            # Calculate MAPE for each available method
            method_mapes = {}
            
            for method, preds in predictions_dict.items():
                if method == 'actual':
                    continue
                
                # Get predictions for this campaign's rows
                indices = group.index
                campaign_preds = [predictions_dict[method][i] for i in range(len(df_val)) if df_val.index[i] in indices]
                
                if len(campaign_preds) > 0:
                    mape = np.mean(np.abs((actual - np.mean(campaign_preds)) / (actual + 1e-6)))
                    method_mapes[method] = mape
            
            # Select best method (lowest MAPE)
            if method_mapes:
                best_method = min(method_mapes, key=method_mapes.get)
                campaign_methods.append({
                    'app_id': app_id,
                    'campaign': campaign,
                    'tier': tier,
                    'best_method': best_method,
                    'mape': method_mapes[best_method]
                })
        
        method_df = pd.DataFrame(campaign_methods)
        
        # Distribution
        print(f"\n  Method distribution:")
        for method, count in method_df['best_method'].value_counts().items():
            pct = count / len(method_df) * 100
            print(f"    - {method}: {count} campaigns ({pct:.1f}%)")
        
        return method_df
    
    def evaluate_final_ensemble(self, df_val, method_selection_df, predictions_dict):
        """Evaluate final ensemble using selected methods"""
        print(f"\n[Evaluation] Final Ensemble Performance...")
        
        # Merge selections
        df_eval = df_val.merge(
            method_selection_df[['app_id', 'campaign', 'best_method']],
            on=['app_id', 'campaign'],
            how='left'
        )
        
        # Default method if not found
        df_eval['best_method'] = df_eval['best_method'].fillna('lookalike')
        
        # Get final predictions (using selected method per campaign)
        final_preds = []
        
        for idx, row in df_eval.iterrows():
            method = row['best_method']
            
            # Find index in predictions_dict
            val_idx = list(df_val.index).index(row.name) if row.name in df_val.index else None
            
            if val_idx is not None and method in predictions_dict:
                final_preds.append(predictions_dict[method][val_idx])
            else:
                final_preds.append(row['ltv_d30'])  # Fallback
        
        df_eval['ltv_pred_ensemble'] = final_preds
        
        # Metrics
        mape = mean_absolute_percentage_error(df_eval['ltv_d30'], df_eval['ltv_pred_ensemble'])
        r2 = r2_score(df_eval['ltv_d30'], df_eval['ltv_pred_ensemble'])
        
        print(f"\n  ‚úì Final Ensemble Metrics:")
        print(f"    - MAPE: {mape:.4f}")
        print(f"    - R¬≤: {r2:.4f}")
        
        return {
            'mape': mape,
            'r2': r2,
            'predictions': df_eval
        }
    
    def save_calibration_params(self, tier, alpha, anchor):
        """Save calibration parameters"""
        self.calibration_params[tier] = {
            'alpha': alpha,
            'anchor': anchor
        }
        
        output_path = Path('models/calibration')
        output_path.mkdir(parents=True, exist_ok=True)
        
        joblib.dump(self.calibration_params, output_path / 'calibration_params.pkl')
        print(f"\n‚úì Saved calibration params to: {output_path}")
    
    def save_method_selection(self, method_df):
        """Save method selection"""
        output_path = Path('models/ensemble')
        output_path.mkdir(parents=True, exist_ok=True)
        
        method_df.to_csv(output_path / 'method_selection.csv', index=False)
        print(f"‚úì Saved method selection to: {output_path}")

def main():
    """Main execution"""
    print("="*60)
    print("STEP 12: CALIBRATION & OPTIMIZATION")
    print("="*60)
    
    # Load validation data
    df_val = pd.read_csv('data/features/validation.csv')
    
    # Initialize
    optimizer = CalibrationOptimizer()
    
    # Process each tier
    all_method_selections = []
    final_results = []
    
    for tier in ['tier1', 'tier2', 'tier3']:
        print(f"\n{'='*60}")
        print(f"OPTIMIZING: {tier.upper()}")
        print(f"{'='*60}")
        
        df_tier = df_val[df_val['tier'] == tier]
        
        if len(df_tier) == 0:
            print(f"  ‚ö† No data for {tier}")
            continue
        
        # Load all predictions
        predictions_dict = optimizer.load_all_model_predictions(df_tier, tier)
        
        # Calculate anchor
        anchor = optimizer.calculate_anchor(df_val, tier)
        
        # Optimize alpha for main method (Hurdle)
        if 'hurdle' in predictions_dict:
            best_alpha, calibrated_preds = optimizer.optimize_alpha(
                df_tier, predictions_dict, anchor, 'hurdle'
            )
            print(f"\n  ‚úì Optimal Œ± (Hurdle): {best_alpha:.2f}")
            
            # Update predictions with calibrated
            if calibrated_preds is not None:
                predictions_dict['hurdle'] = calibrated_preds
            
            # Save calibration params
            optimizer.save_calibration_params(tier, best_alpha, anchor)
        
        # Select best method per campaign
        method_df = optimizer.select_best_method_per_campaign(df_tier, predictions_dict, tier)
        all_method_selections.append(method_df)
        
        # Evaluate ensemble
        results = optimizer.evaluate_final_ensemble(df_tier, method_df, predictions_dict)
        final_results.append({
            'tier': tier,
            'mape': results['mape'],
            'r2': results['r2']
        })
    
    # Combine and save method selections
    all_methods_df = pd.concat(all_method_selections, ignore_index=True)
    optimizer.save_method_selection(all_methods_df)
    
    # Save final summary
    summary_df = pd.DataFrame(final_results)
    summary_df.to_csv('results/step12_final_ensemble_summary.csv', index=False)
    
    print("\n" + "="*60)
    print("‚úÖ STEP 12 COMPLETED!")
    print("="*60)
    print("\nFinal Ensemble Performance:")
    print(summary_df.to_string(index=False))
    print("\nNext Step: step13_production_pipeline.py")

if __name__ == "__main__":
    main()
```

---

## ‚úÖ SUCCESS CRITERIA

- [x] Optimal Œ± found for each tier
- [x] Method selection completed for all campaigns
- [x] Final MAPE ‚â§ 4% (Tier 1), ‚â§ 6% (Tier 2), ‚â§ 10% (Tier 3)
- [x] Calibration params and method selection saved

---

## üéØ NEXT STEP

‚û°Ô∏è **[Step 13: Production Pipeline](step13_production_pipeline.md)**

---

**Estimated Time:** 6-8 hours  
**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê Very Hard
