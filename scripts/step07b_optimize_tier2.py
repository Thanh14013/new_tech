"""
Step 07b: TIER2 MODEL OPTIMIZATION
==================================
Systematic optimization to reduce Tier2 MAPE

Key insight: The original 15.32% MAPE is calculated on all users.
We'll try multiple strategies to improve this.
"""

import pandas as pd
import numpy as np
from pathlib import Path
import pickle
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_absolute_percentage_error, r2_score, mean_absolute_error
from xgboost import XGBClassifier, XGBRegressor
import warnings
warnings.filterwarnings('ignore')


def load_tier2_data():
    """Load Tier2 training and validation data"""
    print("ğŸ“‚ Loading Tier2 data...")
    
    train_df = pd.read_csv('data/features/train.csv')
    val_df = pd.read_csv('data/features/validation.csv')
    
    train_t2 = train_df[train_df['tier'] == 'tier2'].copy()
    val_t2 = val_df[val_df['tier'] == 'tier2'].copy()
    
    print(f"   Train: {len(train_t2):,} rows")
    print(f"   Val:   {len(val_t2):,} rows")
    
    return train_t2, val_t2


def prepare_features(df):
    """Prepare numeric features"""
    exclude_cols = ['ltv_d60', 'ltv_d30', 'tier', 'campaign', 'country', 
                   'platform', 'install_date', 'month', 'month_num']
    
    feature_cols = [c for c in df.columns if c not in exclude_cols]
    
    X = df[feature_cols].copy()
    y = df['ltv_d60'].copy()
    
    # Keep only numeric
    numeric_cols = X.select_dtypes(include=[np.number]).columns
    X = X[numeric_cols].copy()
    
    # Handle inf/nan
    X = X.replace([np.inf, -np.inf], np.nan)
    X = X.fillna(0)
    
    return X, y


def evaluate_model(y_true, y_pred, method_name):
    """Evaluate predictions with MAPE filtering"""
    # Filter out very small LTV values for MAPE calculation
    mask = y_true > 0.01
    
    if mask.sum() > 0:
        mape = mean_absolute_percentage_error(y_true[mask], y_pred[mask])
    else:
        mape = 9.99
    
    r2 = r2_score(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    
    print(f"\nâœ… {method_name} Results:")
    print(f"   MAPE: {mape*100:.2f}%")
    print(f"   RÂ²:   {r2:.4f}")
    print(f"   MAE:  ${mae:.2f}")
    
    return {'method': method_name, 'mape': mape, 'r2': r2, 'mae': mae}


def train_hurdle_model(X_train, y_train, X_val, y_val, params=None):
    """Train two-stage hurdle model with given parameters"""
    
    if params is None:
        params = {
            'n_estimators': 200,
            'max_depth': 6,
            'learning_rate': 0.05,
            'subsample': 0.8,
            'random_state': 42
        }
    
    # Stage 1: Classifier
    is_payer_train = (y_train > 0).astype(int)
    is_payer_val = (y_val > 0).astype(int)
    
    clf = XGBClassifier(eval_metric='auc', **params)
    clf.set_params(early_stopping_rounds=20)
    clf.fit(X_train, is_payer_train,
            eval_set=[(X_val, is_payer_val)],
            verbose=False)
    
    # Stage 2: Regressor (payers only)
    payer_mask_train = y_train > 0
    payer_mask_val = y_val > 0
    
    reg = XGBRegressor(eval_metric='rmse', **params)
    reg.set_params(early_stopping_rounds=20)
    reg.fit(X_train[payer_mask_train], y_train[payer_mask_train],
            eval_set=[(X_val[payer_mask_val], y_val[payer_mask_val])],
            verbose=False)
    
    # Predict
    prob_payer_val = clf.predict_proba(X_val)[:, 1]
    amount_pred_val = np.maximum(0, reg.predict(X_val))
    y_pred_val = prob_payer_val * amount_pred_val
    
    return clf, reg, y_pred_val


def strategy1_baseline(train_df, val_df):
    """Strategy 1: Baseline (current approach)"""
    print("\n" + "="*80)
    print("ğŸ¯ STRATEGY 1: BASELINE")
    print("="*80)
    
    X_train, y_train = prepare_features(train_df)
    X_val, y_val = prepare_features(val_df)
    
    clf, reg, y_pred = train_hurdle_model(X_train, y_train, X_val, y_val)
    
    result = evaluate_model(y_val, y_pred, "Baseline")
    result['clf'] = clf
    result['reg'] = reg
    result['features'] = X_train.columns.tolist()
    
    return result


def strategy2_remove_outliers(train_df, val_df, percentile=99):
    """Strategy 2: Remove extreme outliers from training"""
    print("\n" + "="*80)
    print(f"ğŸ¯ STRATEGY 2: OUTLIER REMOVAL (Q{percentile})")
    print("="*80)
    
    # Remove high outliers
    q_threshold = train_df['ltv_d60'].quantile(percentile / 100)
    train_clean = train_df[train_df['ltv_d60'] <= q_threshold].copy()
    
    print(f"   Threshold: ${q_threshold:.2f}")
    print(f"   Removed: {len(train_df) - len(train_clean):,} rows")
    print(f"   Remaining: {len(train_clean):,} rows")
    
    X_train, y_train = prepare_features(train_clean)
    X_val, y_val = prepare_features(val_df)
    
    clf, reg, y_pred = train_hurdle_model(X_train, y_train, X_val, y_val)
    
    result = evaluate_model(y_val, y_pred, f"Outlier Removal Q{percentile}")
    result['clf'] = clf
    result['reg'] = reg
    result['features'] = X_train.columns.tolist()
    
    return result


def strategy3_hyperparameter_tuning(train_df, val_df):
    """Strategy 3: Hyperparameter tuning with RandomizedSearchCV"""
    print("\n" + "="*80)
    print("ğŸ¯ STRATEGY 3: HYPERPARAMETER TUNING")
    print("="*80)
    
    X_train, y_train = prepare_features(train_df)
    X_val, y_val = prepare_features(val_df)
    
    print("\n   Tuning XGBoost parameters...")
    print("   (This may take several minutes)")
    
    # Stage 1: Optimize classifier
    print("\n   Stage 1: Classifier...")
    is_payer_train = (y_train > 0).astype(int)
    
    clf_params = {
        'n_estimators': [100, 200, 300, 400],
        'max_depth': [4, 6, 8, 10],
        'learning_rate': [0.01, 0.05, 0.1],
        'subsample': [0.7, 0.8, 0.9],
        'colsample_bytree': [0.7, 0.8, 0.9],
        'min_child_weight': [1, 3, 5],
        'gamma': [0, 0.1, 0.2, 0.3]
    }
    
    clf_search = RandomizedSearchCV(
        XGBClassifier(random_state=42, eval_metric='auc'),
        clf_params,
        n_iter=30,
        cv=3,
        scoring='roc_auc',
        random_state=42,
        n_jobs=-1,
        verbose=0
    )
    clf_search.fit(X_train, is_payer_train)
    best_clf = clf_search.best_estimator_
    
    print(f"   Best AUC: {clf_search.best_score_:.4f}")
    
    # Stage 2: Optimize regressor
    print("\n   Stage 2: Regressor...")
    payer_mask_train = y_train > 0
    
    reg_params = {
        'n_estimators': [100, 200, 300, 400],
        'max_depth': [4, 6, 8, 10],
        'learning_rate': [0.01, 0.05, 0.1],
        'subsample': [0.7, 0.8, 0.9],
        'colsample_bytree': [0.7, 0.8, 0.9],
        'min_child_weight': [1, 3, 5],
        'gamma': [0, 0.1, 0.2, 0.3]
    }
    
    reg_search = RandomizedSearchCV(
        XGBRegressor(random_state=42, eval_metric='rmse'),
        reg_params,
        n_iter=30,
        cv=3,
        scoring='neg_mean_absolute_error',
        random_state=42,
        n_jobs=-1,
        verbose=0
    )
    reg_search.fit(X_train[payer_mask_train], y_train[payer_mask_train])
    best_reg = reg_search.best_estimator_
    
    print(f"   Best MAE: ${-reg_search.best_score_:.2f}")
    
    # Predict with best models
    prob_payer_val = best_clf.predict_proba(X_val)[:, 1]
    amount_pred_val = np.maximum(0, best_reg.predict(X_val))
    y_pred = prob_payer_val * amount_pred_val
    
    result = evaluate_model(y_val, y_pred, "Hyperparameter Tuning")
    result['clf'] = best_clf
    result['reg'] = best_reg
    result['features'] = X_train.columns.tolist()
    
    return result


def strategy4_deeper_trees(train_df, val_df):
    """Strategy 4: Deeper trees with more estimators"""
    print("\n" + "="*80)
    print("ğŸ¯ STRATEGY 4: DEEPER TREES + MORE ESTIMATORS")
    print("="*80)
    
    X_train, y_train = prepare_features(train_df)
    X_val, y_val = prepare_features(val_df)
    
    params = {
        'n_estimators': 500,
        'max_depth': 10,
        'learning_rate': 0.03,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'min_child_weight': 3,
        'gamma': 0.1,
        'random_state': 42
    }
    
    print(f"   Parameters: {params}")
    
    clf, reg, y_pred = train_hurdle_model(X_train, y_train, X_val, y_val, params)
    
    result = evaluate_model(y_val, y_pred, "Deeper Trees")
    result['clf'] = clf
    result['reg'] = reg
    result['features'] = X_train.columns.tolist()
    
    return result


def strategy5_regularized(train_df, val_df):
    """Strategy 5: Strong regularization"""
    print("\n" + "="*80)
    print("ğŸ¯ STRATEGY 5: STRONG REGULARIZATION")
    print("="*80)
    
    X_train, y_train = prepare_features(train_df)
    X_val, y_val = prepare_features(val_df)
    
    params = {
        'n_estimators': 300,
        'max_depth': 4,
        'learning_rate': 0.01,
        'subsample': 0.7,
        'colsample_bytree': 0.7,
        'min_child_weight': 5,
        'gamma': 0.3,
        'reg_alpha': 0.1,
        'reg_lambda': 1.0,
        'random_state': 42
    }
    
    print(f"   Parameters: Strong L1/L2 regularization")
    
    clf, reg, y_pred = train_hurdle_model(X_train, y_train, X_val, y_val, params)
    
    result = evaluate_model(y_val, y_pred, "Strong Regularization")
    result['clf'] = clf
    result['reg'] = reg
    result['features'] = X_train.columns.tolist()
    
    return result


def compare_results(results):
    """Compare all strategies"""
    print("\n" + "="*80)
    print("ğŸ“Š STRATEGY COMPARISON")
    print("="*80)
    
    # Sort by MAPE
    results_sorted = sorted(results, key=lambda x: x['mape'])
    
    print(f"\n{'Rank':<6} {'Strategy':<30} {'MAPE':<12} {'RÂ²':<12} {'MAE':<12}")
    print("-" * 80)
    
    for i, result in enumerate(results_sorted, 1):
        marker = "ğŸ†" if i == 1 else f"{i}."
        mape_pct = result['mape'] * 100
        print(f"{marker:<6} {result['method']:<30} {mape_pct:>6.2f}%     {result['r2']:>8.4f}     ${result['mae']:>8.2f}")
    
    # Best model
    best = results_sorted[0]
    baseline = [r for r in results if r['method'] == 'Baseline'][0]
    
    print("\n" + "="*80)
    print("ğŸ† BEST STRATEGY")
    print("="*80)
    print(f"\nStrategy: {best['method']}")
    print(f"MAPE:     {best['mape']*100:.2f}% (target: <6%)")
    print(f"RÂ²:       {best['r2']:.4f}")
    print(f"MAE:      ${best['mae']:.2f}")
    
    improvement = ((baseline['mape'] - best['mape']) / baseline['mape']) * 100
    print(f"\nğŸ“ˆ Improvement: {improvement:.2f}% reduction from baseline")
    
    if best['mape'] < 0.06:
        print(f"\nâœ… TARGET ACHIEVED! MAPE {best['mape']*100:.2f}% < 6%")
    elif best['mape'] < 0.08:
        print(f"\nâœ“ Close to target! MAPE {best['mape']*100:.2f}% < 8%")
    elif best['mape'] < 0.10:
        print(f"\nâš ï¸  Getting closer. MAPE {best['mape']*100:.2f}% < 10%")
    else:
        print(f"\nâš ï¸  Still needs improvement. MAPE {best['mape']*100:.2f}% > 10%")
    
    return best


def save_best_model(best_result):
    """Save the best model"""
    print("\n" + "="*80)
    print("ğŸ’¾ SAVING OPTIMIZED MODEL")
    print("="*80)
    
    output_dir = Path('models/tier2_optimized')
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Save models
    clf_path = output_dir / 'hurdle_stage1_classifier.pkl'
    reg_path = output_dir / 'hurdle_stage2_regressor.pkl'
    features_path = output_dir / 'feature_list.txt'
    
    with open(clf_path, 'wb') as f:
        pickle.dump(best_result['clf'], f)
    
    with open(reg_path, 'wb') as f:
        pickle.dump(best_result['reg'], f)
    
    with open(features_path, 'w') as f:
        for feat in best_result['features']:
            f.write(f"{feat}\n")
    
    print(f"\nâœ… Saved:")
    print(f"   {clf_path}")
    print(f"   {reg_path}")
    print(f"   {features_path}")


def main():
    """Main optimization pipeline"""
    print("="*80)
    print("ğŸ¯ TIER2 MODEL OPTIMIZATION")
    print("="*80)
    print("\nGoal: Reduce MAPE from 15.32% to <6%")
    print("\nTesting 5 optimization strategies:")
    print("  1. Baseline (current)")
    print("  2. Outlier removal")
    print("  3. Hyperparameter tuning")
    print("  4. Deeper trees")
    print("  5. Strong regularization")
    print()
    
    # Load data
    train_df, val_df = load_tier2_data()
    
    # Run all strategies
    results = []
    
    results.append(strategy1_baseline(train_df, val_df))
    results.append(strategy2_remove_outliers(train_df, val_df, percentile=99))
    results.append(strategy2_remove_outliers(train_df, val_df, percentile=95))
    results.append(strategy3_hyperparameter_tuning(train_df, val_df))
    results.append(strategy4_deeper_trees(train_df, val_df))
    results.append(strategy5_regularized(train_df, val_df))
    
    # Compare and select best
    best_result = compare_results(results)
    
    # Save best model
    save_best_model(best_result)
    
    print("\n" + "="*80)
    print("âœ… OPTIMIZATION COMPLETE!")
    print("="*80)
    print()


if __name__ == "__main__":
    main()
